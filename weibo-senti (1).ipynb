{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16bf772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T16:54:08.667347Z",
     "iopub.status.busy": "2022-11-23T16:54:08.666825Z",
     "iopub.status.idle": "2022-11-23T16:54:55.867907Z",
     "shell.execute_reply": "2022-11-23T16:54:55.865946Z"
    },
    "papermill": {
     "duration": 47.210393,
     "end_time": "2022-11-23T16:54:55.871293",
     "exception": false,
     "start_time": "2022-11-23T16:54:08.660900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paddlepaddle-gpu==2.3.2\r\n",
      "  Downloading paddlepaddle_gpu-2.3.2-cp37-cp37m-manylinux1_x86_64.whl (394.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.0/394.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: protobuf<=3.20.0,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (3.19.4)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (1.15.0)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (5.1.1)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (9.1.1)\r\n",
      "Requirement already satisfied: numpy>=1.13 in /opt/conda/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (1.21.6)\r\n",
      "Collecting astor\r\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\r\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in /opt/conda/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (3.3.0)\r\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (2.28.1)\r\n",
      "Collecting paddle-bfloat==0.1.7\r\n",
      "  Downloading paddle_bfloat-0.1.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (394 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.8/394.8 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu==2.3.2) (2022.9.24)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu==2.3.2) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu==2.3.2) (1.26.12)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu==2.3.2) (2.1.0)\r\n",
      "Installing collected packages: paddle-bfloat, astor, paddlepaddle-gpu\r\n",
      "Successfully installed astor-0.8.1 paddle-bfloat-0.1.7 paddlepaddle-gpu-2.3.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install paddlepaddle-gpu==2.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50eb4bf9",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-11-23T16:54:55.947654Z",
     "iopub.status.busy": "2022-11-23T16:54:55.947082Z",
     "iopub.status.idle": "2022-11-23T16:55:19.656678Z",
     "shell.execute_reply": "2022-11-23T16:55:19.655176Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 23.751787,
     "end_time": "2022-11-23T16:55:19.658934",
     "exception": false,
     "start_time": "2022-11-23T16:54:55.907147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paddlenlp==2.4.2\r\n",
      "  Downloading paddlenlp-2.4.2-py3-none-any.whl (1.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from paddlenlp==2.4.2) (2.1.0)\r\n",
      "Collecting seqeval\r\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting multiprocess<=0.70.12.2\r\n",
      "  Downloading multiprocess-0.70.12.2-py37-none-any.whl (112 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.1/112.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting dill<0.3.5\r\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting paddlefsl\r\n",
      "  Downloading paddlefsl-1.1.0-py3-none-any.whl (101 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.0/101.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting paddle2onnx\r\n",
      "  Downloading paddle2onnx-1.0.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from paddlenlp==2.4.2) (0.1.97)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from paddlenlp==2.4.2) (0.4.5)\r\n",
      "Requirement already satisfied: protobuf<=3.20.0,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from paddlenlp==2.4.2) (3.19.4)\r\n",
      "Collecting visualdl\r\n",
      "  Downloading visualdl-2.4.1-py3-none-any.whl (4.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: colorlog in /opt/conda/lib/python3.7/site-packages (from paddlenlp==2.4.2) (6.7.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from paddlenlp==2.4.2) (4.64.0)\r\n",
      "Requirement already satisfied: jieba in /opt/conda/lib/python3.7/site-packages (from paddlenlp==2.4.2) (0.42.1)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (3.8.1)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (2.28.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (1.21.6)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (1.3.5)\r\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (5.0.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (21.3)\r\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (0.18.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (0.10.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (4.13.0)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (2022.8.2)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (3.0.0)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval->paddlenlp==2.4.2) (1.0.2)\r\n",
      "Requirement already satisfied: flask>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from visualdl->paddlenlp==2.4.2) (2.2.2)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from visualdl->paddlenlp==2.4.2) (3.5.3)\r\n",
      "Requirement already satisfied: Pillow>=7.0.0 in /opt/conda/lib/python3.7/site-packages (from visualdl->paddlenlp==2.4.2) (9.1.1)\r\n",
      "Collecting bce-python-sdk\r\n",
      "  Downloading bce_python_sdk-0.8.74-py3-none-any.whl (204 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.6/204.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from visualdl->paddlenlp==2.4.2) (1.15.0)\r\n",
      "Collecting Flask-Babel>=1.0.0\r\n",
      "  Downloading Flask_Babel-2.0.0-py3-none-any.whl (9.3 kB)\r\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.4.2) (2.1.2)\r\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.4.2) (2.2.2)\r\n",
      "Requirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.4.2) (3.1.2)\r\n",
      "Requirement already satisfied: click>=8.0 in /opt/conda/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.4.2) (8.0.4)\r\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp==2.4.2) (2.10.3)\r\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp==2.4.2) (2022.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (21.4.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (1.3.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (1.7.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (1.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (4.1.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (4.0.2)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (6.0.2)\r\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (0.13.0)\r\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (2.1.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp==2.4.2) (6.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp==2.4.2) (3.7.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets>=2.0.0->paddlenlp==2.4.2) (3.8.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets>=2.0.0->paddlenlp==2.4.2) (3.0.9)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.4.2) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.4.2) (1.26.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.4.2) (2022.9.24)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.4.2) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.4.2) (1.0.1)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.4.2) (1.7.3)\r\n",
      "Collecting pycryptodome>=3.8.0\r\n",
      "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: future>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp==2.4.2) (0.18.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp==2.4.2) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp==2.4.2) (4.33.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp==2.4.2) (2.8.2)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp==2.4.2) (1.4.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=3.0->flask>=1.1.1->visualdl->paddlenlp==2.4.2) (2.1.1)\r\n",
      "Building wheels for collected packages: seqeval\r\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=5a21b6d80926485b64f4489b3a7bc0b4211a123b3e5c2262c2bdf3a6e13884bb\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\r\n",
      "Successfully built seqeval\r\n",
      "Installing collected packages: paddle2onnx, pycryptodome, dill, paddlefsl, multiprocess, bce-python-sdk, seqeval, Flask-Babel, visualdl, paddlenlp\r\n",
      "  Attempting uninstall: dill\r\n",
      "    Found existing installation: dill 0.3.5.1\r\n",
      "    Uninstalling dill-0.3.5.1:\r\n",
      "      Successfully uninstalled dill-0.3.5.1\r\n",
      "  Attempting uninstall: multiprocess\r\n",
      "    Found existing installation: multiprocess 0.70.13\r\n",
      "    Uninstalling multiprocess-0.70.13:\r\n",
      "      Successfully uninstalled multiprocess-0.70.13\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pathos 0.2.9 requires dill>=0.3.5.1, but you have dill 0.3.4 which is incompatible.\r\n",
      "pathos 0.2.9 requires multiprocess>=0.70.13, but you have multiprocess 0.70.12.2 which is incompatible.\r\n",
      "apache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed Flask-Babel-2.0.0 bce-python-sdk-0.8.74 dill-0.3.4 multiprocess-0.70.12.2 paddle2onnx-1.0.3 paddlefsl-1.1.0 paddlenlp-2.4.2 pycryptodome-3.15.0 seqeval-1.2.2 visualdl-2.4.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install paddlenlp==2.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a24fca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T16:55:19.723086Z",
     "iopub.status.busy": "2022-11-23T16:55:19.722741Z",
     "iopub.status.idle": "2022-11-23T16:55:22.817751Z",
     "shell.execute_reply": "2022-11-23T16:55:22.816756Z"
    },
    "papermill": {
     "duration": 3.124823,
     "end_time": "2022-11-23T16:55:22.820123",
     "exception": false,
     "start_time": "2022-11-23T16:55:19.695300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import paddle\n",
    "import paddlenlp\n",
    "import pandas as pd\n",
    "from paddlenlp.datasets import MapDataset\n",
    "import functools\n",
    "import numpy as np\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "import time\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77b50bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T16:55:22.873446Z",
     "iopub.status.busy": "2022-11-23T16:55:22.872848Z",
     "iopub.status.idle": "2022-11-23T16:55:22.880272Z",
     "shell.execute_reply": "2022-11-23T16:55:22.879324Z"
    },
    "papermill": {
     "duration": 0.035952,
     "end_time": "2022-11-23T16:55:22.882285",
     "exception": false,
     "start_time": "2022-11-23T16:55:22.846333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建验证集evaluate函数\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        \n",
    "    accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accuracy: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eac3904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T16:55:22.934138Z",
     "iopub.status.busy": "2022-11-23T16:55:22.933796Z",
     "iopub.status.idle": "2022-11-23T16:55:24.754201Z",
     "shell.execute_reply": "2022-11-23T16:55:24.753033Z"
    },
    "papermill": {
     "duration": 1.853027,
     "end_time": "2022-11-23T16:55:24.760335",
     "exception": false,
     "start_time": "2022-11-23T16:55:22.907308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据类型: <class 'paddlenlp.datasets.dataset.MapDataset'>\n",
      "训练集样例: {'text': '回覆嗯~ 很幸福！以后我也要这样……今晚和一个本地人去了一个cafe ，到了之后他和我说这儿的老板是一对GAY ，然后我跑去看这一对，哎，好幸福啊！一起幸福的经营者一家超有feel 的cafe ，羡慕死了！今晚很开心！', 'label': 1, 'qid': '1'}\n",
      "验证集样例: {'text': '书读多了未必都是好事呀，很多人都教条主义了，变成书呆子一个啊。。。。笑死我了！好一对新生代对联！愿读服输争当精品，是珍珠，必会发光。好一对新生代对联！博士生，研究生，本科生，生生不息！下联：上一届，这一届，下一届，届届失业！横批：愿读服输via 财经在线', 'label': 1, 'qid': '1'}\n",
      "测试集样例: {'text': '我在看：【保姆纵火案4年后那个痛失妻儿的男人怎么样了】#跟新浪看热点#O保姆纵火案4年后那个痛失妻儿的男人怎么样了', 'label': '', 'qid': '1'}\n"
     ]
    }
   ],
   "source": [
    "labelmap={\"none\":0,\"happiness\":1,\"sadness\":2,\"anger\":3,\"surprise\":4,\"fear\":5,\"disgust\":6}\n",
    "traindata = pd.read_csv('/kaggle/input/datav4/train.csv', sep=None, header=0, encoding='utf-8', engine='python')\n",
    "train_ds=MapDataset([{'text':d.text,'label':labelmap[d.label],'qid':str(d.qid)} for d in traindata.itertuples()])\n",
    "devdata = pd.read_csv('/kaggle/input/datav4/test.csv', sep=None, header=0, encoding='utf-8', engine='python')\n",
    "dev_ds=MapDataset([{'text':d.text,'label':labelmap[d.label],'qid':str(d.qid)} for d in devdata.itertuples()])\n",
    "testdata = pd.read_csv('/kaggle/input/datav4/dev.csv', sep=None, header=0, encoding='utf-8', engine='python')\n",
    "test_ds=MapDataset([{'text':d.text,'label':\"\",'qid':str(d.qid)} for d in testdata.itertuples()])\n",
    "# 数据集返回为MapDataset类型\n",
    "print(\"数据类型:\", type(train_ds))\n",
    "# label代表标签，qid代表数据编号，测试集中不包含标签信息\n",
    "print(\"训练集样例:\", train_ds[0])\n",
    "print(\"验证集样例:\", dev_ds[0])\n",
    "print(\"测试集样例:\", test_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03a6a3bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T16:55:24.815017Z",
     "iopub.status.busy": "2022-11-23T16:55:24.814636Z",
     "iopub.status.idle": "2022-11-23T16:57:55.501008Z",
     "shell.execute_reply": "2022-11-23T16:57:55.499971Z"
    },
    "papermill": {
     "duration": 150.785964,
     "end_time": "2022-11-23T16:57:55.573576",
     "exception": false,
     "start_time": "2022-11-23T16:55:24.787612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-11-23 16:55:24,816] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load 'ernie-3.0-xbase-zh'.\u001b[0m\n",
      "\u001b[32m[2022-11-23 16:55:24,818] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_xbase_zh.pdparams and saved to /root/.paddlenlp/models/ernie-3.0-xbase-zh\u001b[0m\n",
      "\u001b[32m[2022-11-23 16:55:24,820] [    INFO]\u001b[0m - Downloading ernie_3.0_xbase_zh.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_xbase_zh.pdparams\u001b[0m\n",
      "100%|██████████| 1.11G/1.11G [02:15<00:00, 8.76MB/s]\n",
      "W1123 16:57:42.105593    24 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 6.0, Driver API Version: 11.4, Runtime API Version: 10.2\n",
      "W1123 16:57:42.152628    24 gpu_resources.cc:91] device: 0, cuDNN Version: 8.0.\n",
      "\u001b[32m[2022-11-23 16:57:50,267] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-xbase-zh'.\u001b[0m\n",
      "\u001b[32m[2022-11-23 16:57:50,269] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_xbase_zh_vocab.txt and saved to /root/.paddlenlp/models/ernie-3.0-xbase-zh\u001b[0m\n",
      "\u001b[32m[2022-11-23 16:57:50,271] [    INFO]\u001b[0m - Downloading ernie_3.0_xbase_zh_vocab.txt from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_xbase_zh_vocab.txt\u001b[0m\n",
      "100%|██████████| 182k/182k [00:03<00:00, 51.8kB/s]\n",
      "\u001b[32m[2022-11-23 16:57:55,494] [    INFO]\u001b[0m - tokenizer config file saved in /root/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-11-23 16:57:55,495] [    INFO]\u001b[0m - Special tokens file saved in /root/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ernie-3.0-xbase-zh\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=7)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7696a842",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T16:57:55.720729Z",
     "iopub.status.busy": "2022-11-23T16:57:55.720327Z",
     "iopub.status.idle": "2022-11-23T16:57:55.729203Z",
     "shell.execute_reply": "2022-11-23T16:57:55.728197Z"
    },
    "papermill": {
     "duration": 0.08453,
     "end_time": "2022-11-23T16:57:55.731278",
     "exception": false,
     "start_time": "2022-11-23T16:57:55.646748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "from paddlenlp.data import DataCollatorWithPadding\n",
    "\n",
    "# 数据预处理函数，利用分词器将文本转化为整数序列\n",
    "def preprocess_function(examples, tokenizer, max_seq_length, is_test=False):\n",
    "\n",
    "    result = tokenizer(text=examples[\"text\"], max_seq_len=max_seq_length)\n",
    "    if not is_test:\n",
    "        result[\"labels\"] = examples[\"label\"]\n",
    "    return result\n",
    "\n",
    "trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=128)\n",
    "train_ds = train_ds.map(trans_func)\n",
    "dev_ds = dev_ds.map(trans_func)\n",
    "\n",
    "# collate_fn函数构造，将不同长度序列充到批中数据的最大长度，再将数据堆叠\n",
    "collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader\n",
    "train_batch_sampler = BatchSampler(train_ds, batch_size=16, shuffle=True)\n",
    "dev_batch_sampler = BatchSampler(dev_ds, batch_size=16, shuffle=False)\n",
    "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
    "dev_data_loader = DataLoader(dataset=dev_ds, batch_sampler=dev_batch_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8162ea5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T16:57:55.875729Z",
     "iopub.status.busy": "2022-11-23T16:57:55.875347Z",
     "iopub.status.idle": "2022-11-23T16:57:55.882809Z",
     "shell.execute_reply": "2022-11-23T16:57:55.881622Z"
    },
    "papermill": {
     "duration": 0.081731,
     "end_time": "2022-11-23T16:57:55.884823",
     "exception": false,
     "start_time": "2022-11-23T16:57:55.803092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adam优化器、交叉熵损失函数、accuracy评价指标\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67168c85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-23T16:57:56.029348Z",
     "iopub.status.busy": "2022-11-23T16:57:56.028981Z",
     "iopub.status.idle": "2022-11-24T01:37:13.187377Z",
     "shell.execute_reply": "2022-11-24T01:37:13.186236Z"
    },
    "papermill": {
     "duration": 31157.233468,
     "end_time": "2022-11-24T01:37:13.189919",
     "exception": false,
     "start_time": "2022-11-23T16:57:55.956451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 200, epoch: 1, batch: 200, loss: 0.58825, accu: 0.60000, speed: 1.86 step/s\n",
      "global step 400, epoch: 1, batch: 400, loss: 1.03246, accu: 0.65734, speed: 1.90 step/s\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.73723, accu: 0.69073, speed: 1.92 step/s\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.59192, accu: 0.69875, speed: 1.89 step/s\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.71657, accu: 0.71256, speed: 1.89 step/s\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.54276, accu: 0.72141, speed: 1.89 step/s\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.81225, accu: 0.72817, speed: 1.90 step/s\n",
      "global step 1600, epoch: 1, batch: 1600, loss: 0.85994, accu: 0.73426, speed: 1.89 step/s\n",
      "global step 1800, epoch: 1, batch: 1800, loss: 0.35955, accu: 0.73958, speed: 1.90 step/s\n",
      "global step 2000, epoch: 1, batch: 2000, loss: 0.35070, accu: 0.74506, speed: 1.88 step/s\n",
      "2000 eval loss: 0.65221, accuracy: 0.77799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-11-23 17:22:22,245] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt1669224139.6267614/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-11-23 17:22:22,251] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt1669224139.6267614/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 2200, epoch: 1, batch: 2200, loss: 0.62946, accu: 0.78531, speed: 0.39 step/s\n",
      "global step 2400, epoch: 1, batch: 2400, loss: 0.61989, accu: 0.79688, speed: 1.89 step/s\n",
      "global step 2600, epoch: 1, batch: 2600, loss: 0.20337, accu: 0.79250, speed: 1.91 step/s\n",
      "global step 2800, epoch: 1, batch: 2800, loss: 0.57439, accu: 0.79391, speed: 1.87 step/s\n",
      "global step 3000, epoch: 1, batch: 3000, loss: 0.72443, accu: 0.79200, speed: 1.89 step/s\n",
      "global step 3200, epoch: 1, batch: 3200, loss: 0.49517, accu: 0.79141, speed: 1.90 step/s\n",
      "global step 3400, epoch: 1, batch: 3400, loss: 0.33227, accu: 0.79250, speed: 1.88 step/s\n",
      "global step 3600, epoch: 1, batch: 3600, loss: 0.36389, accu: 0.79297, speed: 1.89 step/s\n",
      "global step 3800, epoch: 1, batch: 3800, loss: 0.73057, accu: 0.79260, speed: 1.89 step/s\n",
      "global step 4000, epoch: 1, batch: 4000, loss: 0.68889, accu: 0.79375, speed: 1.89 step/s\n",
      "4000 eval loss: 0.57994, accuracy: 0.80903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-11-23 17:46:48,726] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt1669225605.887588/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-11-23 17:46:48,731] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt1669225605.887588/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 4200, epoch: 1, batch: 4200, loss: 0.42890, accu: 0.80375, speed: 0.39 step/s\n",
      "global step 4400, epoch: 1, batch: 4400, loss: 0.21524, accu: 0.79484, speed: 1.89 step/s\n",
      "global step 4600, epoch: 1, batch: 4600, loss: 0.47163, accu: 0.79656, speed: 1.91 step/s\n",
      "global step 4800, epoch: 1, batch: 4800, loss: 0.16440, accu: 0.79898, speed: 1.90 step/s\n",
      "global step 5000, epoch: 1, batch: 5000, loss: 0.90844, accu: 0.80044, speed: 1.89 step/s\n",
      "global step 5200, epoch: 1, batch: 5200, loss: 0.85715, accu: 0.80010, speed: 1.90 step/s\n",
      "global step 5400, epoch: 1, batch: 5400, loss: 0.27794, accu: 0.80107, speed: 1.90 step/s\n",
      "global step 5600, epoch: 1, batch: 5600, loss: 0.11480, accu: 0.79945, speed: 1.89 step/s\n",
      "global step 5800, epoch: 1, batch: 5800, loss: 0.54374, accu: 0.79833, speed: 1.90 step/s\n",
      "global step 6000, epoch: 1, batch: 6000, loss: 0.70385, accu: 0.79809, speed: 1.90 step/s\n",
      "6000 eval loss: 0.54025, accuracy: 0.81771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-11-23 18:11:10,500] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt1669227067.7638109/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-11-23 18:11:10,503] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt1669227067.7638109/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 6200, epoch: 1, batch: 6200, loss: 0.27965, accu: 0.79625, speed: 0.39 step/s\n",
      "global step 6400, epoch: 1, batch: 6400, loss: 0.72106, accu: 0.79641, speed: 1.88 step/s\n",
      "global step 6600, epoch: 1, batch: 6600, loss: 0.42528, accu: 0.79906, speed: 1.88 step/s\n",
      "global step 6800, epoch: 1, batch: 6800, loss: 0.40361, accu: 0.79812, speed: 1.88 step/s\n",
      "global step 7000, epoch: 1, batch: 7000, loss: 0.20247, accu: 0.79900, speed: 1.90 step/s\n",
      "global step 7200, epoch: 1, batch: 7200, loss: 0.80501, accu: 0.80042, speed: 1.89 step/s\n",
      "global step 7400, epoch: 1, batch: 7400, loss: 0.79526, accu: 0.80018, speed: 1.92 step/s\n",
      "global step 7600, epoch: 1, batch: 7600, loss: 0.79360, accu: 0.80098, speed: 1.91 step/s\n",
      "global step 7800, epoch: 1, batch: 7800, loss: 0.40436, accu: 0.80253, speed: 1.89 step/s\n",
      "global step 8000, epoch: 1, batch: 8000, loss: 0.24544, accu: 0.80409, speed: 1.88 step/s\n",
      "8000 eval loss: 0.52256, accuracy: 0.82340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-11-23 18:35:31,911] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt1669228529.2517617/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-11-23 18:35:31,914] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt1669228529.2517617/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 8200, epoch: 1, batch: 8200, loss: 0.95629, accu: 0.80625, speed: 0.39 step/s\n",
      "global step 8400, epoch: 1, batch: 8400, loss: 0.55419, accu: 0.81172, speed: 1.89 step/s\n",
      "global step 8600, epoch: 1, batch: 8600, loss: 0.68794, accu: 0.80990, speed: 1.92 step/s\n",
      "global step 8800, epoch: 2, batch: 191, loss: 0.47074, accu: 0.81897, speed: 1.90 step/s\n",
      "global step 9000, epoch: 2, batch: 391, loss: 0.39585, accu: 0.82262, speed: 1.90 step/s\n",
      "global step 9200, epoch: 2, batch: 591, loss: 0.26934, accu: 0.82739, speed: 1.91 step/s\n",
      "global step 9400, epoch: 2, batch: 791, loss: 0.38895, accu: 0.83018, speed: 1.91 step/s\n",
      "global step 9600, epoch: 2, batch: 991, loss: 0.22957, accu: 0.83086, speed: 1.90 step/s\n",
      "global step 9800, epoch: 2, batch: 1191, loss: 0.47120, accu: 0.83191, speed: 1.89 step/s\n",
      "global step 10000, epoch: 2, batch: 1391, loss: 0.32068, accu: 0.83325, speed: 1.90 step/s\n",
      "10000 eval loss: 0.56036, accuracy: 0.81372\n",
      "global step 10200, epoch: 2, batch: 1591, loss: 0.41910, accu: 0.85219, speed: 0.39 step/s\n",
      "global step 10400, epoch: 2, batch: 1791, loss: 0.44703, accu: 0.85141, speed: 1.89 step/s\n",
      "global step 10600, epoch: 2, batch: 1991, loss: 0.42995, accu: 0.84740, speed: 1.89 step/s\n",
      "global step 10800, epoch: 2, batch: 2191, loss: 1.03624, accu: 0.84258, speed: 1.91 step/s\n",
      "global step 11000, epoch: 2, batch: 2391, loss: 0.49149, accu: 0.84100, speed: 1.90 step/s\n",
      "global step 11200, epoch: 2, batch: 2591, loss: 0.33169, accu: 0.84099, speed: 1.91 step/s\n",
      "global step 11400, epoch: 2, batch: 2791, loss: 0.13354, accu: 0.84214, speed: 1.88 step/s\n",
      "global step 11600, epoch: 2, batch: 2991, loss: 0.13151, accu: 0.84160, speed: 1.90 step/s\n",
      "global step 11800, epoch: 2, batch: 3191, loss: 0.57816, accu: 0.84090, speed: 1.92 step/s\n",
      "global step 12000, epoch: 2, batch: 3391, loss: 0.40361, accu: 0.84141, speed: 1.90 step/s\n",
      "12000 eval loss: 0.54015, accuracy: 0.82716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-11-23 19:24:07,434] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt1669231444.754276/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-11-23 19:24:07,436] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt1669231444.754276/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 12200, epoch: 2, batch: 3591, loss: 0.53443, accu: 0.84937, speed: 0.39 step/s\n",
      "global step 12400, epoch: 2, batch: 3791, loss: 0.38156, accu: 0.84531, speed: 1.91 step/s\n",
      "global step 12600, epoch: 2, batch: 3991, loss: 0.55776, accu: 0.84521, speed: 1.93 step/s\n",
      "global step 12800, epoch: 2, batch: 4191, loss: 0.49583, accu: 0.84578, speed: 1.90 step/s\n",
      "global step 13000, epoch: 2, batch: 4391, loss: 0.82489, accu: 0.84444, speed: 1.90 step/s\n",
      "global step 13200, epoch: 2, batch: 4591, loss: 0.08823, accu: 0.84354, speed: 1.90 step/s\n",
      "global step 13400, epoch: 2, batch: 4791, loss: 0.15964, accu: 0.84254, speed: 1.91 step/s\n",
      "global step 13600, epoch: 2, batch: 4991, loss: 0.24635, accu: 0.84230, speed: 1.90 step/s\n",
      "global step 13800, epoch: 2, batch: 5191, loss: 0.72128, accu: 0.84208, speed: 1.89 step/s\n",
      "global step 14000, epoch: 2, batch: 5391, loss: 0.71905, accu: 0.84166, speed: 1.90 step/s\n",
      "14000 eval loss: 0.53134, accuracy: 0.81900\n",
      "global step 14200, epoch: 2, batch: 5591, loss: 0.26501, accu: 0.85406, speed: 0.39 step/s\n",
      "global step 14400, epoch: 2, batch: 5791, loss: 0.05662, accu: 0.84859, speed: 1.90 step/s\n",
      "global step 14600, epoch: 2, batch: 5991, loss: 0.42192, accu: 0.84406, speed: 1.91 step/s\n",
      "global step 14800, epoch: 2, batch: 6191, loss: 0.63545, accu: 0.84070, speed: 1.88 step/s\n",
      "global step 15000, epoch: 2, batch: 6391, loss: 0.44011, accu: 0.83862, speed: 1.91 step/s\n",
      "global step 15200, epoch: 2, batch: 6591, loss: 0.71224, accu: 0.83875, speed: 1.90 step/s\n",
      "global step 15400, epoch: 2, batch: 6791, loss: 0.59752, accu: 0.83830, speed: 1.89 step/s\n",
      "global step 15600, epoch: 2, batch: 6991, loss: 0.17907, accu: 0.83852, speed: 1.89 step/s\n",
      "global step 15800, epoch: 2, batch: 7191, loss: 0.06025, accu: 0.83976, speed: 1.90 step/s\n",
      "global step 16000, epoch: 2, batch: 7391, loss: 0.25363, accu: 0.84025, speed: 1.89 step/s\n",
      "16000 eval loss: 0.52338, accuracy: 0.82911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-11-23 20:12:43,063] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt1669234360.372969/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-11-23 20:12:43,068] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt1669234360.372969/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 16200, epoch: 2, batch: 7591, loss: 0.25442, accu: 0.83688, speed: 0.39 step/s\n",
      "global step 16400, epoch: 2, batch: 7791, loss: 0.47601, accu: 0.83594, speed: 1.90 step/s\n",
      "global step 16600, epoch: 2, batch: 7991, loss: 0.46992, accu: 0.83521, speed: 1.89 step/s\n",
      "global step 16800, epoch: 2, batch: 8191, loss: 0.52331, accu: 0.83648, speed: 1.89 step/s\n",
      "global step 17000, epoch: 2, batch: 8391, loss: 0.53172, accu: 0.83913, speed: 1.90 step/s\n",
      "global step 17200, epoch: 2, batch: 8591, loss: 0.47768, accu: 0.83958, speed: 1.91 step/s\n",
      "global step 17400, epoch: 3, batch: 182, loss: 0.22967, accu: 0.84554, speed: 1.92 step/s\n",
      "global step 17600, epoch: 3, batch: 382, loss: 0.75563, accu: 0.84942, speed: 1.91 step/s\n",
      "global step 17800, epoch: 3, batch: 582, loss: 0.37811, accu: 0.85306, speed: 1.91 step/s\n",
      "global step 18000, epoch: 3, batch: 782, loss: 0.28553, accu: 0.85529, speed: 1.91 step/s\n",
      "18000 eval loss: 0.57166, accuracy: 0.82462\n",
      "global step 18200, epoch: 3, batch: 982, loss: 0.47825, accu: 0.87469, speed: 0.39 step/s\n",
      "global step 18400, epoch: 3, batch: 1182, loss: 0.11473, accu: 0.87766, speed: 1.89 step/s\n",
      "global step 18600, epoch: 3, batch: 1382, loss: 0.20743, accu: 0.88010, speed: 1.91 step/s\n",
      "global step 18800, epoch: 3, batch: 1582, loss: 0.23461, accu: 0.87672, speed: 1.90 step/s\n",
      "global step 19000, epoch: 3, batch: 1782, loss: 0.26765, accu: 0.87644, speed: 1.91 step/s\n",
      "global step 19200, epoch: 3, batch: 1982, loss: 0.22936, accu: 0.87578, speed: 1.89 step/s\n",
      "global step 19400, epoch: 3, batch: 2182, loss: 0.50722, accu: 0.87705, speed: 1.89 step/s\n",
      "global step 19600, epoch: 3, batch: 2382, loss: 0.11723, accu: 0.87707, speed: 1.88 step/s\n",
      "global step 19800, epoch: 3, batch: 2582, loss: 0.08484, accu: 0.87576, speed: 1.90 step/s\n",
      "global step 20000, epoch: 3, batch: 2782, loss: 0.27708, accu: 0.87534, speed: 1.88 step/s\n",
      "20000 eval loss: 0.55538, accuracy: 0.82322\n",
      "global step 20200, epoch: 3, batch: 2982, loss: 0.31688, accu: 0.87719, speed: 0.39 step/s\n",
      "global step 20400, epoch: 3, batch: 3182, loss: 0.03266, accu: 0.87250, speed: 1.90 step/s\n",
      "global step 20600, epoch: 3, batch: 3382, loss: 0.72856, accu: 0.87354, speed: 1.89 step/s\n",
      "global step 20800, epoch: 3, batch: 3582, loss: 0.75620, accu: 0.87250, speed: 1.90 step/s\n",
      "global step 21000, epoch: 3, batch: 3782, loss: 0.19607, accu: 0.87175, speed: 1.89 step/s\n",
      "global step 21200, epoch: 3, batch: 3982, loss: 0.12267, accu: 0.87375, speed: 1.91 step/s\n",
      "global step 21400, epoch: 3, batch: 4182, loss: 0.19247, accu: 0.87379, speed: 1.89 step/s\n",
      "global step 21600, epoch: 3, batch: 4382, loss: 0.45401, accu: 0.87520, speed: 1.89 step/s\n",
      "global step 21800, epoch: 3, batch: 4582, loss: 0.30041, accu: 0.87438, speed: 1.90 step/s\n",
      "global step 22000, epoch: 3, batch: 4782, loss: 0.71899, accu: 0.87362, speed: 1.91 step/s\n",
      "22000 eval loss: 0.55707, accuracy: 0.82439\n",
      "global step 22200, epoch: 3, batch: 4982, loss: 0.81528, accu: 0.86562, speed: 0.39 step/s\n",
      "global step 22400, epoch: 3, batch: 5182, loss: 0.49491, accu: 0.86422, speed: 1.90 step/s\n",
      "global step 22600, epoch: 3, batch: 5382, loss: 0.26864, accu: 0.86990, speed: 1.90 step/s\n",
      "global step 22800, epoch: 3, batch: 5582, loss: 0.17983, accu: 0.86836, speed: 1.90 step/s\n",
      "global step 23000, epoch: 3, batch: 5782, loss: 0.23662, accu: 0.87138, speed: 1.91 step/s\n",
      "global step 23200, epoch: 3, batch: 5982, loss: 0.65110, accu: 0.87375, speed: 1.90 step/s\n",
      "global step 23400, epoch: 3, batch: 6182, loss: 0.47229, accu: 0.87384, speed: 1.89 step/s\n",
      "global step 23600, epoch: 3, batch: 6382, loss: 0.06315, accu: 0.87270, speed: 1.92 step/s\n",
      "global step 23800, epoch: 3, batch: 6582, loss: 0.25206, accu: 0.87319, speed: 1.90 step/s\n",
      "global step 24000, epoch: 3, batch: 6782, loss: 0.41788, accu: 0.87247, speed: 1.89 step/s\n",
      "24000 eval loss: 0.54758, accuracy: 0.82576\n",
      "global step 24200, epoch: 3, batch: 6982, loss: 0.15346, accu: 0.87687, speed: 0.39 step/s\n",
      "global step 24400, epoch: 3, batch: 7182, loss: 0.31804, accu: 0.87453, speed: 1.91 step/s\n",
      "global step 24600, epoch: 3, batch: 7382, loss: 0.21466, accu: 0.87281, speed: 1.90 step/s\n",
      "global step 24800, epoch: 3, batch: 7582, loss: 0.31572, accu: 0.87461, speed: 1.90 step/s\n",
      "global step 25000, epoch: 3, batch: 7782, loss: 0.91348, accu: 0.87269, speed: 1.90 step/s\n",
      "global step 25200, epoch: 3, batch: 7982, loss: 0.31885, accu: 0.87339, speed: 1.89 step/s\n",
      "global step 25400, epoch: 3, batch: 8182, loss: 0.34919, accu: 0.87263, speed: 1.90 step/s\n",
      "global step 25600, epoch: 3, batch: 8382, loss: 0.53490, accu: 0.87164, speed: 1.90 step/s\n",
      "global step 25800, epoch: 3, batch: 8582, loss: 0.41833, accu: 0.87083, speed: 1.88 step/s\n",
      "global step 26000, epoch: 4, batch: 173, loss: 0.33186, accu: 0.87473, speed: 1.90 step/s\n",
      "26000 eval loss: 0.57148, accuracy: 0.82759\n",
      "global step 26200, epoch: 4, batch: 373, loss: 0.18736, accu: 0.91500, speed: 0.39 step/s\n",
      "global step 26400, epoch: 4, batch: 573, loss: 0.23243, accu: 0.91234, speed: 1.90 step/s\n",
      "global step 26600, epoch: 4, batch: 773, loss: 0.30741, accu: 0.90844, speed: 1.92 step/s\n",
      "global step 26800, epoch: 4, batch: 973, loss: 0.32011, accu: 0.90766, speed: 1.89 step/s\n",
      "global step 27000, epoch: 4, batch: 1173, loss: 0.25906, accu: 0.90700, speed: 1.91 step/s\n",
      "global step 27200, epoch: 4, batch: 1373, loss: 0.15116, accu: 0.90635, speed: 1.91 step/s\n",
      "global step 27400, epoch: 4, batch: 1573, loss: 0.24581, accu: 0.90522, speed: 1.92 step/s\n",
      "global step 27600, epoch: 4, batch: 1773, loss: 0.29794, accu: 0.90629, speed: 1.88 step/s\n",
      "global step 27800, epoch: 4, batch: 1973, loss: 0.09325, accu: 0.90642, speed: 1.88 step/s\n",
      "global step 28000, epoch: 4, batch: 2173, loss: 0.28411, accu: 0.90697, speed: 1.88 step/s\n",
      "28000 eval loss: 0.59590, accuracy: 0.82890\n",
      "global step 28200, epoch: 4, batch: 2373, loss: 0.12002, accu: 0.90000, speed: 0.39 step/s\n",
      "global step 28400, epoch: 4, batch: 2573, loss: 0.14664, accu: 0.90094, speed: 1.89 step/s\n",
      "global step 28600, epoch: 4, batch: 2773, loss: 0.43623, accu: 0.90010, speed: 1.88 step/s\n",
      "global step 28800, epoch: 4, batch: 2973, loss: 0.44068, accu: 0.89906, speed: 1.89 step/s\n",
      "global step 29000, epoch: 4, batch: 3173, loss: 0.39808, accu: 0.90094, speed: 1.90 step/s\n",
      "global step 29200, epoch: 4, batch: 3373, loss: 0.19427, accu: 0.90068, speed: 1.90 step/s\n",
      "global step 29400, epoch: 4, batch: 3573, loss: 0.01710, accu: 0.90138, speed: 1.90 step/s\n",
      "global step 29600, epoch: 4, batch: 3773, loss: 0.42583, accu: 0.90023, speed: 1.90 step/s\n",
      "global step 29800, epoch: 4, batch: 3973, loss: 0.22775, accu: 0.89965, speed: 1.89 step/s\n",
      "global step 30000, epoch: 4, batch: 4173, loss: 0.67222, accu: 0.89912, speed: 1.89 step/s\n",
      "30000 eval loss: 0.58363, accuracy: 0.82826\n",
      "global step 30200, epoch: 4, batch: 4373, loss: 0.36486, accu: 0.90125, speed: 0.39 step/s\n",
      "global step 30400, epoch: 4, batch: 4573, loss: 0.17287, accu: 0.90344, speed: 1.91 step/s\n",
      "global step 30600, epoch: 4, batch: 4773, loss: 0.24593, accu: 0.90260, speed: 1.90 step/s\n",
      "global step 30800, epoch: 4, batch: 4973, loss: 0.61333, accu: 0.90148, speed: 1.90 step/s\n",
      "global step 31000, epoch: 4, batch: 5173, loss: 0.01378, accu: 0.90112, speed: 1.90 step/s\n",
      "global step 31200, epoch: 4, batch: 5373, loss: 0.21293, accu: 0.90099, speed: 1.90 step/s\n",
      "global step 31400, epoch: 4, batch: 5573, loss: 0.11458, accu: 0.90156, speed: 1.88 step/s\n",
      "global step 31600, epoch: 4, batch: 5773, loss: 0.13777, accu: 0.90266, speed: 1.90 step/s\n",
      "global step 31800, epoch: 4, batch: 5973, loss: 0.25508, accu: 0.90205, speed: 1.89 step/s\n",
      "global step 32000, epoch: 4, batch: 6173, loss: 0.08117, accu: 0.90197, speed: 1.90 step/s\n",
      "32000 eval loss: 0.59849, accuracy: 0.82538\n",
      "global step 32200, epoch: 4, batch: 6373, loss: 0.09179, accu: 0.89281, speed: 0.39 step/s\n",
      "global step 32400, epoch: 4, batch: 6573, loss: 0.23548, accu: 0.89578, speed: 1.93 step/s\n",
      "global step 32600, epoch: 4, batch: 6773, loss: 0.39139, accu: 0.89542, speed: 1.91 step/s\n",
      "global step 32800, epoch: 4, batch: 6973, loss: 0.49128, accu: 0.89414, speed: 1.90 step/s\n",
      "global step 33000, epoch: 4, batch: 7173, loss: 0.45387, accu: 0.89525, speed: 1.91 step/s\n",
      "global step 33200, epoch: 4, batch: 7373, loss: 0.17560, accu: 0.89594, speed: 1.91 step/s\n",
      "global step 33400, epoch: 4, batch: 7573, loss: 0.15310, accu: 0.89656, speed: 1.91 step/s\n",
      "global step 33600, epoch: 4, batch: 7773, loss: 0.95148, accu: 0.89523, speed: 1.88 step/s\n",
      "global step 33800, epoch: 4, batch: 7973, loss: 0.31881, accu: 0.89469, speed: 1.92 step/s\n",
      "global step 34000, epoch: 4, batch: 8173, loss: 0.43468, accu: 0.89497, speed: 1.92 step/s\n",
      "34000 eval loss: 0.57157, accuracy: 0.82832\n",
      "global step 34200, epoch: 4, batch: 8373, loss: 0.25109, accu: 0.89094, speed: 0.39 step/s\n",
      "global step 34400, epoch: 4, batch: 8573, loss: 0.47349, accu: 0.89516, speed: 1.89 step/s\n",
      "global step 34600, epoch: 5, batch: 164, loss: 0.14378, accu: 0.90384, speed: 1.91 step/s\n",
      "global step 34800, epoch: 5, batch: 364, loss: 0.13536, accu: 0.90992, speed: 1.91 step/s\n",
      "global step 35000, epoch: 5, batch: 564, loss: 0.13293, accu: 0.91456, speed: 1.90 step/s\n",
      "global step 35200, epoch: 5, batch: 764, loss: 0.31530, accu: 0.91667, speed: 1.90 step/s\n",
      "global step 35400, epoch: 5, batch: 964, loss: 0.42143, accu: 0.91844, speed: 1.91 step/s\n",
      "global step 35600, epoch: 5, batch: 1164, loss: 0.15853, accu: 0.91949, speed: 1.91 step/s\n",
      "global step 35800, epoch: 5, batch: 1364, loss: 0.09379, accu: 0.91990, speed: 1.91 step/s\n",
      "global step 36000, epoch: 5, batch: 1564, loss: 0.03715, accu: 0.92113, speed: 1.88 step/s\n",
      "36000 eval loss: 0.63125, accuracy: 0.82214\n",
      "global step 36200, epoch: 5, batch: 1764, loss: 0.32550, accu: 0.92000, speed: 0.39 step/s\n",
      "global step 36400, epoch: 5, batch: 1964, loss: 0.39479, accu: 0.92141, speed: 1.90 step/s\n",
      "global step 36600, epoch: 5, batch: 2164, loss: 0.05658, accu: 0.92427, speed: 1.91 step/s\n",
      "global step 36800, epoch: 5, batch: 2364, loss: 0.51645, accu: 0.92367, speed: 1.89 step/s\n",
      "global step 37000, epoch: 5, batch: 2564, loss: 0.16188, accu: 0.92463, speed: 1.90 step/s\n",
      "global step 37200, epoch: 5, batch: 2764, loss: 0.22212, accu: 0.92432, speed: 1.91 step/s\n",
      "global step 37400, epoch: 5, batch: 2964, loss: 0.08207, accu: 0.92415, speed: 1.88 step/s\n",
      "global step 37600, epoch: 5, batch: 3164, loss: 0.41605, accu: 0.92336, speed: 1.91 step/s\n",
      "global step 37800, epoch: 5, batch: 3364, loss: 0.11324, accu: 0.92326, speed: 1.91 step/s\n",
      "global step 38000, epoch: 5, batch: 3564, loss: 0.01471, accu: 0.92328, speed: 1.92 step/s\n",
      "38000 eval loss: 0.63244, accuracy: 0.81809\n",
      "global step 38200, epoch: 5, batch: 3764, loss: 0.18939, accu: 0.91719, speed: 0.39 step/s\n",
      "global step 38400, epoch: 5, batch: 3964, loss: 0.58130, accu: 0.91734, speed: 1.90 step/s\n",
      "global step 38600, epoch: 5, batch: 4164, loss: 0.53833, accu: 0.91833, speed: 1.90 step/s\n",
      "global step 38800, epoch: 5, batch: 4364, loss: 0.26636, accu: 0.91953, speed: 1.91 step/s\n",
      "global step 39000, epoch: 5, batch: 4564, loss: 1.03254, accu: 0.91969, speed: 1.90 step/s\n",
      "global step 39200, epoch: 5, batch: 4764, loss: 0.10550, accu: 0.91969, speed: 1.91 step/s\n",
      "global step 39400, epoch: 5, batch: 4964, loss: 0.85180, accu: 0.92004, speed: 1.89 step/s\n",
      "global step 39600, epoch: 5, batch: 5164, loss: 0.04144, accu: 0.92004, speed: 1.89 step/s\n",
      "global step 39800, epoch: 5, batch: 5364, loss: 0.21304, accu: 0.91882, speed: 1.90 step/s\n",
      "global step 40000, epoch: 5, batch: 5564, loss: 0.36057, accu: 0.91822, speed: 1.90 step/s\n",
      "40000 eval loss: 0.64062, accuracy: 0.81614\n",
      "global step 40200, epoch: 5, batch: 5764, loss: 0.25105, accu: 0.91094, speed: 0.39 step/s\n",
      "global step 40400, epoch: 5, batch: 5964, loss: 0.07335, accu: 0.91500, speed: 1.89 step/s\n",
      "global step 40600, epoch: 5, batch: 6164, loss: 0.76702, accu: 0.91708, speed: 1.88 step/s\n",
      "global step 40800, epoch: 5, batch: 6364, loss: 0.52691, accu: 0.91750, speed: 1.91 step/s\n",
      "global step 41000, epoch: 5, batch: 6564, loss: 0.03427, accu: 0.91625, speed: 1.92 step/s\n",
      "global step 41200, epoch: 5, batch: 6764, loss: 0.18521, accu: 0.91635, speed: 1.90 step/s\n",
      "global step 41400, epoch: 5, batch: 6964, loss: 0.08209, accu: 0.91656, speed: 1.90 step/s\n",
      "global step 41600, epoch: 5, batch: 7164, loss: 0.45359, accu: 0.91652, speed: 1.89 step/s\n",
      "global step 41800, epoch: 5, batch: 7364, loss: 0.70373, accu: 0.91639, speed: 1.91 step/s\n",
      "global step 42000, epoch: 5, batch: 7564, loss: 0.02153, accu: 0.91572, speed: 1.90 step/s\n",
      "42000 eval loss: 0.65962, accuracy: 0.82302\n",
      "global step 42200, epoch: 5, batch: 7764, loss: 0.45799, accu: 0.91594, speed: 0.39 step/s\n",
      "global step 42400, epoch: 5, batch: 7964, loss: 0.18647, accu: 0.91687, speed: 1.92 step/s\n",
      "global step 42600, epoch: 5, batch: 8164, loss: 0.23678, accu: 0.91698, speed: 1.90 step/s\n",
      "global step 42800, epoch: 5, batch: 8364, loss: 0.16131, accu: 0.91625, speed: 1.89 step/s\n",
      "global step 43000, epoch: 5, batch: 8564, loss: 0.32157, accu: 0.91744, speed: 1.91 step/s\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "import time\n",
    "import paddle.nn.functional as F\n",
    "epochs = 5 # 训练轮次\n",
    "ckpt_dir = \"ernie_ckpt\" #训练过程中保存模型参数的文件夹\n",
    "best_acc = 0\n",
    "best_step = 0\n",
    "bestmodel=ckpt_dir\n",
    "global_step = 0 #迭代次数\n",
    "tic_train = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "\n",
    "        # 计算模型输出、损失函数值、分类概率值、准确率\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        # 每迭代100次，打印损失函数值、准确率、计算速度\n",
    "        global_step += 1\n",
    "        if global_step % 200 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    200 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        # 反向梯度回传，更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        # 每迭代1500次，评估当前训练的模型、保存当前模型参数和分词器的词表等\n",
    "        if global_step % 2000 == 0:\n",
    "            save_dir = ckpt_dir\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            print(global_step, end=' ')\n",
    "            acc_eval = evaluate(model, criterion, metric, dev_data_loader)\n",
    "            if acc_eval > best_acc:\n",
    "                best_acc = acc_eval\n",
    "                best_step = global_step\n",
    "                t=time.time()\n",
    "                bestmodel=save_dir+str(t)\n",
    "                model.save_pretrained(save_dir+str(t))\n",
    "                tokenizer.save_pretrained(save_dir+str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8eaa5d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T01:37:13.359230Z",
     "iopub.status.busy": "2022-11-24T01:37:13.358591Z",
     "iopub.status.idle": "2022-11-24T01:43:58.132644Z",
     "shell.execute_reply": "2022-11-24T01:43:58.131733Z"
    },
    "papermill": {
     "duration": 404.975984,
     "end_time": "2022-11-24T01:43:58.250611",
     "exception": false,
     "start_time": "2022-11-24T01:37:13.274627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在dev集表现 eval loss: 0.52338, accuracy: 0.82911\n"
     ]
    }
   ],
   "source": [
    "# 加载ERNIR 3.0最佳模型参数\n",
    "params_path = './'+bestmodel+'/model_state.pdparams'\n",
    "state_dict = paddle.load(params_path)\n",
    "model.set_dict(state_dict)\n",
    "print('在dev集表现', end=' ')\n",
    "eval_acc = evaluate(model, criterion, metric, dev_data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b1678a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T01:43:58.458485Z",
     "iopub.status.busy": "2022-11-24T01:43:58.458153Z",
     "iopub.status.idle": "2022-11-24T01:54:35.954574Z",
     "shell.execute_reply": "2022-11-24T01:54:35.953643Z"
    },
    "papermill": {
     "duration": 637.585598,
     "end_time": "2022-11-24T01:54:35.957784",
     "exception": false,
     "start_time": "2022-11-24T01:43:58.372186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n"
     ]
    }
   ],
   "source": [
    "# 测试集数据预处理，利用分词器将文本转化为整数序列\n",
    "trans_func_test = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=128, is_test=True)\n",
    "test_ds_trans = test_ds.map(trans_func_test)\n",
    "\n",
    "# 进行采样组batch\n",
    "collate_fn_test = DataCollatorWithPadding(tokenizer)\n",
    "test_batch_sampler = BatchSampler(test_ds_trans, batch_size=16, shuffle=False)\n",
    "test_data_loader = DataLoader(dataset=test_ds_trans, batch_sampler=test_batch_sampler, collate_fn=collate_fn_test)\n",
    "\n",
    "# 模型预测分类结果\n",
    "label_map={0:\"none\",1:\"happiness\",2:\"sadness\",3:\"anger\",4:\"surprise\",5:\"fear\",6:\"disgust\"}\n",
    "results = []\n",
    "probs_for_8=[]\n",
    "model.eval()\n",
    "k=0\n",
    "for batch in test_data_loader:\n",
    "    if k%200==0:\n",
    "        print(k)  \n",
    "    input_ids, token_type_ids = batch['input_ids'], batch['token_type_ids']\n",
    "    logits = model(batch['input_ids'], batch['token_type_ids'])\n",
    "    probs = F.softmax(logits, axis=-1)\n",
    "    #存储标签\n",
    "    idx = paddle.argmax(probs, axis=1).numpy()\n",
    "    idx = idx.tolist()\n",
    "    preds = [label_map[i] for i in idx]\n",
    "    results.extend(preds)\n",
    "    #存储可能性\n",
    "    probs=probs.numpy()\n",
    "    probs=probs.tolist()\n",
    "    probs_for_8.extend([i for i in probs])\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46ed124f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-24T01:54:36.219657Z",
     "iopub.status.busy": "2022-11-24T01:54:36.219114Z",
     "iopub.status.idle": "2022-11-24T01:54:36.236119Z",
     "shell.execute_reply": "2022-11-24T01:54:36.233634Z"
    },
    "papermill": {
     "duration": 0.150311,
     "end_time": "2022-11-24T01:54:36.238471",
     "exception": true,
     "start_time": "2022-11-24T01:54:36.088160",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (468556500.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_24/468556500.py\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    f.write(test_ds[i]['qid']+\",\"+test_ds[i]['text']+\",\"+pred+\",\"+\\\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# 存储预测结果  \n",
    "test_ds=MapDataset([{'text':d.text,'label':\"\",'qid':str(d.qid)} for d in testdata.itertuples()])\n",
    "\n",
    "res_dir = \"./results\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)\n",
    "with open(os.path.join(res_dir, \"weibosenti\"+str(time.time())+\".csv\"), 'w', encoding=\"utf8\") as f:\n",
    "    f.write(\"qid,text,prediction,none,happiness,sadness,anger,surprise,fear,disgust\\n\")\n",
    "    for i, pred in enumerate(results):\n",
    "    f.write(test_ds[i]['qid']+\",\"+test_ds[i]['text']+\",\"+pred+\",\"+\\\n",
    "        str(probs_for_8[i][0])+\",\"+str(probs_for_8[i][1])+\",\"+str(probs_for_8[i][2])+\",\"+str(probs_for_8[i][3])+\",\"+\\\n",
    "        str(probs_for_8[i][4])+\",\"+str(probs_for_8[i][5])+\",\"+str(probs_for_8[i][6])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a191d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47347219",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427943d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32436.713258,
   "end_time": "2022-11-24T01:54:38.060429",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-23T16:54:01.347171",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
